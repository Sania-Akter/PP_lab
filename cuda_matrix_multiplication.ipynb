{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU3p_BlUnrDJ",
        "outputId": "df3a9673-3d8d-4efb-9bac-21fc82760546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile matrix.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void matrixMul(float *A, float *B, float *R, int M, int N, int P, int batchOffset) {\n",
        "  int k = threadIdx.x + batchOffset;\n",
        "  float *a = A + k * M * N;\n",
        "  float *b = B + k * N * P;\n",
        "  float *r = R + k * M * P;\n",
        "for(int outer = 0; outer < 100; outer++) {\n",
        "  for(int i = 0; i < M; i++) {\n",
        "    for(int l = 0; l < P; l++) {\n",
        "      r[i * P + l] = 0.0f; // explicitly set to 0\n",
        "      for(int j = 0; j < N; j++) {\n",
        "        r[i * P + l] += a[i * N + j] * b[j * P + l];\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "}\n",
        "\n",
        "// print the first matrix only\n",
        "void printMatrix(float *A, int M, int N) {\n",
        "  for(int i = 0; i < M; i++) {\n",
        "    for(int j = 0; j < N; j++) {\n",
        "      printf(\" %.0f \", A[i * N + j]);\n",
        "    }\n",
        "    cout<<endl;\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "  int threads = atoi(argv[1]);\n",
        "  int K = atoi(argv[2]);\n",
        "  int M = atoi(argv[3]);\n",
        "  int N = atoi(argv[4]);\n",
        "  int P = atoi(argv[5]);\n",
        "\n",
        "  //int K = 2, M = 2, N = 2, P = 2;\n",
        "\n",
        "  int size_of_a = K * M * N;\n",
        "  int size_of_b = K * N * P;\n",
        "  int size_of_r = K * M * P;\n",
        "\n",
        "  float *h_A = (float*)malloc(size_of_a * sizeof(float));\n",
        "  float *h_B = (float*)malloc(size_of_b * sizeof(float));\n",
        "  float *h_R = (float*)malloc(size_of_r * sizeof(float));\n",
        "\n",
        "  for(int i = 0; i < size_of_a; i++) {\n",
        "    h_A[i] = rand() % 10;\n",
        "  }\n",
        "  for(int i = 0; i < size_of_b; i++) {\n",
        "    h_B[i] = rand() % 10;\n",
        "  }\n",
        "  float *d_A;\n",
        "  cudaMalloc(&d_A, size_of_a * sizeof(float));\n",
        "  cudaMemcpy(d_A, h_A, size_of_a * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  float *d_B;\n",
        "  cudaMalloc(&d_B, size_of_b * sizeof(float));\n",
        "  cudaMemcpy(d_B, h_B, size_of_b * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "  float *d_R;\n",
        "  cudaMalloc(&d_R, size_of_r * sizeof(float));\n",
        "  cudaMemset(d_R, 0, size_of_r * sizeof(float));\n",
        "\n",
        "  int remainingMatrices = K;\n",
        "  int batchOffset = 0;\n",
        "\n",
        "  while(remainingMatrices > 0) {\n",
        "    int currentBatchSize = min(remainingMatrices, threads);\n",
        "    matrixMul<<<1, currentBatchSize>>>(d_A, d_B, d_R, M, N, P, batchOffset);\n",
        "    cudaDeviceSynchronize();\n",
        "    remainingMatrices -= currentBatchSize;\n",
        "    batchOffset += currentBatchSize;\n",
        "  }\n",
        "\n",
        "  cudaMemcpy(h_R, d_R, size_of_r * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "  int index=4;\n",
        "  cout<<\"Matrix A[5]:\"<<endl;\n",
        "  printMatrix(h_A+index, M, N);\n",
        "  cout<<\"Matrix B[5]:\"<<endl;\n",
        "  printMatrix(h_B+index, N, P);\n",
        "  cout<<\"Matrix R[5]:\"<<endl;\n",
        "  printMatrix(h_R, M, P);\n",
        "  cudaFree(d_A);\n",
        "  cudaFree(d_B);\n",
        "  cudaFree(d_R);\n",
        "  free(h_A);\n",
        "  free(h_B);\n",
        "  free(h_R);\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "//!nvcc -arch=sm_75 matrix.cu -o matrix\n",
        "//!time ./matrix 400 2 2 2 2 > output.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 matrix.cu -o matrix"
      ],
      "metadata": {
        "id": "Cqbfplit6NJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mpirun --allow-run-as-root -np 2 ./matrix_mpi"
      ],
      "metadata": {
        "id": "oiizUD64ovIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0baf9ec7-79a8-4d04-8eba-8ce01460f61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------\n",
            "There are not enough slots available in the system to satisfy the 2\n",
            "slots that were requested by the application:\n",
            "\n",
            "  ./matrix_mpi\n",
            "\n",
            "Either request fewer slots for your application, or make more slots\n",
            "available for use.\n",
            "\n",
            "A \"slot\" is the Open MPI term for an allocatable unit where we can\n",
            "launch a process.  The number of slots available are defined by the\n",
            "environment in which Open MPI processes are run:\n",
            "\n",
            "  1. Hostfile, via \"slots=N\" clauses (N defaults to number of\n",
            "     processor cores if not provided)\n",
            "  2. The --host command line parameter, via a \":N\" suffix on the\n",
            "     hostname (N defaults to 1 if not provided)\n",
            "  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)\n",
            "  4. If none of a hostfile, the --host command line parameter, or an\n",
            "     RM is present, Open MPI defaults to the number of processor cores\n",
            "\n",
            "In all the above cases, if you want Open MPI to default to the number\n",
            "of hardware threads instead of the number of processor cores, use the\n",
            "--use-hwthread-cpus option.\n",
            "\n",
            "Alternatively, you can use the --oversubscribe option to ignore the\n",
            "number of available slots when deciding the number of processes to\n",
            "launch.\n",
            "--------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!time ./matrix 400 2 2 2 2 > output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoCpnszL6Xe3",
        "outputId": "49d12394-10cd-4be0-966f-c0ab5c574b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "real\t0m0.177s\n",
            "user\t0m0.012s\n",
            "sys\t0m0.117s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 matrix.cu -o matrix\n"
      ],
      "metadata": {
        "id": "h0nfedYOojy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix 400 3 3 3 3 > output.txt"
      ],
      "metadata": {
        "id": "CFq5hYGWopip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}